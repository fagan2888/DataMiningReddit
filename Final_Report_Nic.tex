
\documentclass[12pt]{article}
\usepackage[margin=.9in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,caption,subcaption,subfig}
\usepackage{graphics}
\usepackage[mathscr]{euscript}
\raggedright
\parindent = 0 cm
\parskip = 8pt
\numberwithin{equation}{section}

\begin{document}
\title{Reddit or Not\\ Final Report \\ Reddit }
\author{Mathew Arndt, Nicolas Bertagnolli and Micheal Matheny}
\date{}
\maketitle
\pagenumbering{arabic}
\newpage
  
\section*{Introduction}
		We had a change in direction after we realized that working with the windmill data would  be more difficult than we had imagined.  The data was so large that we could not effectively manage it in memory. This meant that our project would focus on streaming algorithms or matrix sketching.  We decided that it might be more fun to actually look at aspects of the data instead of how to work with really really big data.  This led us to switch data sets.  \newline

		This project now uses reddit data which is collected in real-time from current posts.  \newline
		
		Reddit is a very rich site where users are allowed to post just about anything.  We are interested in looking at relationships between the users of reddit and their posts.  The hope is that we might be able to uncover interesting relationships that could lead to better modeling of internet forums.  Our analysis consists of examining three different questions. The first question is, "How similar are high ranking posts?"  To asses this we use Jaccard Similarity of n-grams.  If we can uncover some degree of similarity between high ranking posts we might be able to predict if a post is going to get a lot of likes. Next we ask, "Do users post in similar categories?"  To test this idea we cluster the data based on users to see if similar users post in the same subreddits.  This will allow us to look at relationships among users.  This could lead to answer questions like is user A similar to user B.  It could also be used to create a low dimensional embedding of the users, similar to word vectors.  This might be helpful as a pretraining step in some larger learning system. Lastly we ask, "Are there words that appear frequently in specific subreddits." To test this we run Misra-Gris on the data and remove common words like "the," "and," "but," etc.  This will tell us if specific areas in reddit use common diction.  With this we might be able to identify the important words that categorize most of reddit's usage.
		
		
\section*{N-Gram Analysis (Nicolas Bertagnolli)}
	To answer the question, "how similar are high ranking posts?" we take a set of 1000 reddit posts that have over 100 up-votes from all of reddit.  The threshold was determined using domain knowledge of reddit to adequately capture a reasonably "good" post.  We validated this number by calculating the average number of up-votes on a random sample of reddit posts which turned out to be 97 up-votes.  This says that when we take posts that score above 100 we are taking posts that are above average.   The (1-4)-grams for these 1000 posts were then found and the pairwise jaccard similarity was calculated.  This number was then averaged and examined.  We found that the average Jaccard similarity between high ranking reddit posts for different n-grams was:\newline
	\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .025 & $2.24e^{-3}$ & $1.75e^{-3}$ & $1.57e^{-3}$
	  \end{tabular}
	\end{table}
	
	These numbers are very low but they make sense because there is a great deal of variability in high ranking posts on reddit.  Take these two for example:\newline
	
	"Politics and opinions of the law aside, when a citizen has a complaint, they are encouraged to contact their representatives.  This sort of dickhead response deserves public shaming. Of course, he'll surely claim his email was hacked."\newline
	
	and \newline
	
	"If only he had another one"
	
	The first is fairly long , and the second is only a few words.  This in and of itself ensures low similarity just based on size.  Not to mention, that they both don't share a single word.  As it stands now we can say that the words alone do not make good predictors of reddit likeability.  However, there was a lot of variance in the rankings of these 1000 posts.  What happens if we restrict the posts to a range and refine our question say, "How similar are high ranking posts where the rankings differ by no more than 100 votes?"  Here we examine what happens when we take reddit posts that received between 100 and 200 up-votes.  We find that the similarities are:\newline
	
\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .023 & $2.21e^{-3}$ & $1.69e^{-3}$ & $1.54e^{-3}$
	  \end{tabular}
	\end{table}
	This was surprising because it was slightly worse than the whole set.  This lead us to believe that a measurable amount of the similarity came from the upper upper echelon of reddit posts.This led us to examine posts that received more than 1000 votes.  We find that there is significantly more similarity among posts above 1000 up-votes.  These results are:\newline
	
	\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .042 & .017 & .017 & .016
	  \end{tabular}
	\end{table}
	
	Now In these past experiments all possible subreddits were sampled.  This gives us an idea of similarity across all of reddit, but it seems like there is a lot of variability.  We want to see if we can better understand similarity between highly liked posts in a specific category.  For example, the posts in the funny category are probably more similar to each other than a mixture of posts from funny and say the science reddit.  We find that for the subreddits funny and science the similarities are:
	
		\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Funny & .038 & .017 & .015 & .014\\
	  Science & .494 & .397 & .394 & .394\\
	  \end{tabular}
	\end{table}
	However the Science category only had 5 results that were greater than 100
	
These results are pretty good.  They are almost on par with the above 1000 upvotes for all of reddit.  We now repeat that experiment to see if the similarity improves for extremely well liked posts the results are:

	\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Funny & .130 & .107 & .100 & .085\\
	  \end{tabular}
	\end{table}
	
	
	

\end{document}