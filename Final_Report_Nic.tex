
\documentclass[12pt]{article}
\usepackage[margin=.9in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,caption,subcaption,subfig}
\usepackage{graphics}
\usepackage[mathscr]{euscript}
\raggedright
\parindent = 0in
\parskip = 8pt
\numberwithin{equation}{section}

\begin{document}
\title{Data Mining \\ Final Report \\ Reddit }
\author{Mathew Arndt, Nicolas Bertagnolli and Micheal Matheny}
\date{}
\maketitle
\pagenumbering{arabic}
\newpage
  
\section*{Introduction}
		We had a change in direction after we realized that working with the windmill data would  be more difficult than we had imagined.  The data was so large that we could not effectively manage it in memory. This mean that our project would focus on streaming algorithms or matrix sketching.  We decided that it might be more fun to actually look at aspects of the data instead of how to work with really really big data.  This led us to switch data sets.  We are now using reddit data which is collected in real-time from current posts.  We run three different algorithms on the data to answer three separate questions. The first question is, "How similar are high ranking posts?"  To asses this we use Jaccard Similarity of n-grams.  Next we ask, ""....1
		
		
\section*{N-Gram Analysis}
	To answer the question, "how similar are high ranking posts?" we take a set of 1000 of reddit posts that have over 100 up-votes.  The threshold was determined using domain knowledge of reddit to adequately capture a reasonably "good" post.  We validated this number by calculating the average number of up-votes on a random sample of reddit posts which turned out to be 97 up-votes.  This says that when we take posts that score above 100 we are taking posts that are above average.   The n-grams for these 1000 posts were then found and the pairwise jaccard similarity was calculated.  This number was then averaged and examined.  We found that the average Jaccard similarity between high ranking reddit posts for different n-grams was:\newline
	\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .025 & $2.24e^{-3}$ & $1.75e^{-3}$ & $1.57e^{-3}$
	  \end{tabular}
	\end{table}
	
	These numbers are very low but they make sense because there is a great deal of variability in high ranking posts on reddit.  Take these two for example:\newline
	
	"Politics and opinions of the law aside, when a citizen has a complaint, they are encouraged to contact their representatives.  This sort of dickhead response deserves public shaming. Of course, he'll surely claim his email was hacked."\newline
	
	and \newline
	
	"If only he had another one"
	
	The first is fairly long , and the second is only a few words.  This in and of itself ensures low similarity just based on size.  Not to mention, that they both don't share a single word.  As it stands now we can say that the words alone do not make good predictors of reddit likeability.  However there was a lot of variance in the rankings of these 1000 posts.  What happens if we restrict the posts to a range and refine our question say, "How similar are high ranking posts where the rankings differ by no more than 100 votes?"  Here we examine what happens when we take reddit posts that received between 100 and 200 up-votes.  We find that the similarities are:\newline
	
\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .023 & $2.21e^{-3}$ & $1.69e^{-3}$ & $1.54e^{-3}$
	  \end{tabular}
	\end{table}
	This was surprising because it was slightly worse than the whole set.  Perhaps in the upper upper echelon of reddit posts there is some similarity.  What about if we only look at posts that received more than 1000 votes?  Is there some similarity here?  We find that there is significantly more similarity in the posts among posts above 1000 up-votes.  These results are:\newline
	
	\begin{table}[h!]
	  \begin{tabular}{c | c c c c}
	  n-gram & 1 & 2 & 3 & 4\\
	  \hline
	  Similarity & .042 & .017 & .017 & .016
	  \end{tabular}
	\end{table}
	

\end{document}